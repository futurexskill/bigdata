# -*- coding: utf-8 -*-
"""spark_sql.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CWDDTj1uyqQtKaT4nQdtED0qTjhNtmvJ
"""

# Install PySpark in Google Colab
!pip install pyspark

# Import required libraries
from pyspark.sql import SparkSession

# Create Spark Session
spark = SparkSession.builder \
    .appName("RetailPro Analytics") \
    .master("local[*]") \
    .getOrCreate()

# Verify Spark is working
print(f"Spark Version: {spark.version}")
print("Spark session created successfully!")

# Download the CSV file from GitHub
!wget https://raw.githubusercontent.com/futurexskill/bigdata/master/ecommerce_orders.csv

orders_df = spark.read.csv("ecommerce_orders.csv", header=True, inferSchema=True)

print("âœ… Data loaded!")
orders_df.show(5)

# Create Temp View
create_view_code =  orders_df.createOrReplaceTempView("orders")

# Code Cell 4: Simple SELECT
query1 = """
SELECT product_name, category, total_amount
FROM orders
LIMIT 10
"""

result1 = spark.sql(query1)
result1.show()

# Code Cell 5: WHERE Clause
query2 = """
SELECT product_name, category, total_amount, country
FROM orders
WHERE total_amount > 500
ORDER BY total_amount DESC
"""

result2 = spark.sql(query2)
print("Orders over $500:")
result2.show()

# Code Cell 6: GROUP BY
query3 = """
SELECT
    category,
    COUNT(*) as order_count,
    SUM(total_amount) as total_revenue,
    AVG(total_amount) as avg_order_value,
    MAX(total_amount) as max_order
FROM orders
GROUP BY category
ORDER BY total_revenue DESC
"""

result3 = spark.sql(query3)
print("Revenue by Category:")
result3.show()

# Code Cell 7: Create Second DataFrame and View
from pyspark.sql import Row

category_data = [
    Row(category="Electronics", category_code="ELE", margin_percent=15),
    Row(category="Furniture", category_code="FUR", margin_percent=25),
    Row(category="Stationery", category_code="STA", margin_percent=30),
    Row(category="Accessories", category_code="ACC", margin_percent=20)
]

category_df = spark.createDataFrame(category_data)
category_df.createOrReplaceTempView("categories")

print("âœ… Category lookup view created!")
category_df.show()



# Code Cell 8: JOIN Query
query4 = """
SELECT
    o.category,
    c.category_code,
    COUNT(*) as order_count,
    SUM(o.total_amount) as revenue,
    c.margin_percent,
    ROUND(SUM(o.total_amount) * c.margin_percent / 100, 2) as estimated_profit
FROM orders o
JOIN categories c ON o.category = c.category
GROUP BY o.category, c.category_code, c.margin_percent
ORDER BY estimated_profit DESC
"""

result4 = spark.sql(query4)
print("Profit Analysis by Category:")
result4.show()

# Code Cell 9: Subquery
query5 = """
SELECT product_name, category, total_amount
FROM orders
WHERE total_amount > (
    SELECT AVG(total_amount) FROM orders
)
ORDER BY total_amount DESC
"""

result5 = spark.sql(query5)
print("Products above average price:")
result5.show()

# Code Cell 10: CTE (WITH clause)
query6 = """
WITH country_revenue AS (
    SELECT
        country,
        COUNT(*) as order_count,
        SUM(total_amount) as total_revenue
    FROM orders
    GROUP BY country
),
avg_revenue AS (
    SELECT AVG(total_revenue) as avg_rev
    FROM country_revenue
)
SELECT
    cr.country,
    cr.order_count,
    cr.total_revenue,
    ar.avg_rev as average_revenue,
    ROUND(cr.total_revenue - ar.avg_rev, 2) as diff_from_avg
FROM country_revenue cr
CROSS JOIN avg_revenue ar
WHERE cr.total_revenue > ar.avg_rev
ORDER BY cr.total_revenue DESC
"""

result6 = spark.sql(query6)
print("Countries with above-average revenue:")
result6.show()

# Code Cell 11: DataFrame API vs SQL Comparison
from pyspark.sql.functions import sum, count

# Approach 1: DataFrame API
df_result = orders_df.groupBy("category").agg(
    count("*").alias("order_count"),
    sum("total_amount").alias("total_revenue")
).orderBy("total_revenue", ascending=False)

print("Using DataFrame API:")
df_result.show()

# Approach 2: SQL
sql_result = spark.sql("""
    SELECT
        category,
        COUNT(*) as order_count,
        SUM(total_amount) as total_revenue
    FROM orders
    GROUP BY category
    ORDER BY total_revenue DESC
""")

print("\nUsing SQL:")
sql_result.show()

print("\nâœ… Both produce identical results!")

# Code Cell 12: Global Temporary View
orders_df.createGlobalTempView("global_orders")

# Access global view (note the global_temp prefix)
result = spark.sql("SELECT * FROM global_temp.global_orders LIMIT 5")
print("Global view query:")
result.show()

print("\nâœ… Global views accessible across sessions with global_temp prefix")

# Code Cell 13: List Views
print("Current temporary views:")
spark.catalog.listTables()

# Code Cell 14: Drop Views
spark.catalog.dropTempView("orders")
print("âœ… Temporary view 'orders' dropped")

print("\nRemaining views:")
spark.catalog.listTables()

# Code Cell 15: Real-World Example - Daily Sales Report
# Recreate view for final example
orders_df.createOrReplaceTempView("orders")

sales_report = spark.sql("""
    WITH daily_summary AS (
        SELECT
            category,
            country,
            COUNT(*) as orders,
            SUM(total_amount) as revenue,
            AVG(total_amount) as avg_order_value
        FROM orders
        GROUP BY category, country
    )
    SELECT
        category,
        country,
        orders,
        ROUND(revenue, 2) as revenue,
        ROUND(avg_order_value, 2) as avg_order_value,
        ROUND(revenue * 100.0 / SUM(revenue) OVER (PARTITION BY category), 2) as pct_of_category
    FROM daily_summary
    ORDER BY category, revenue DESC
""")

print("ðŸ“Š Daily Sales Report:")
sales_report.show(20, truncate=False)

# Save report to CSV
sales_report.coalesce(1).write.mode("overwrite").csv("daily_sales_report.csv", header=True)
print("\nâœ… Report saved to daily_sales_report.csv")

