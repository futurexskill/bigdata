# -*- coding: utf-8 -*-
"""pyspark_lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uShpeKeoBd3fZic1xJHI6dDGeLcWKf1c
"""

# Install PySpark in Google Colab
!pip install pyspark

# Import required libraries
from pyspark.sql import SparkSession

# Create Spark Session
spark = SparkSession.builder \
    .appName("RetailPro Analytics") \
    .master("local[*]") \
    .getOrCreate()

# Verify Spark is working
print(f"Spark Version: {spark.version}")
print("Spark session created successfully!")

# Download the CSV file from GitHub
!wget https://raw.githubusercontent.com/futurexskill/bigdata/master/ecommerce_orders.csv

# Load the downloaded CSV file into a Spark DataFrame
orders_df = spark.read.csv(
    "ecommerce_orders.csv",
    header=True,
    inferSchema=True
)

# Display the first few rows
orders_df.show(5)

# Show the schema
orders_df.printSchema()

# Count total records
print(f"Total orders: {orders_df.count()}")

print(f"Total orders in dataset: {orders_df.count()}")

unique_customers = orders_df.select("customer_id").distinct().count()
print(f"Unique customers: {unique_customers}")

from pyspark.sql.functions import min, max

date_range = orders_df.select(
    min("order_date").alias("earliest_order"),
    max("order_date").alias("latest_order")
)
date_range.show()

orders_df.groupBy("category").count().orderBy("count", ascending=False).show()

orders_df.groupBy("country").count().orderBy("count", ascending=False).show()

from pyspark.sql.functions import col, sum as _sum, when

# Check for nulls in each column
orders_df.select([
    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)
    for c in orders_df.columns
]).show()

orders_df.printSchema()

total_orders = orders_df.count()
print(f"Total orders: {total_orders}")

orders_df.describe().show()

from pyspark.sql.functions import col

high_value_orders = orders_df.filter(col("total_amount") > 500)
high_value_orders.show(10)

electronics_high_value = orders_df.filter(
    (col("total_amount") > 500) &
    (col("total_amount") < 1000) &
    (col("category") == "Electronics")
)
electronics_high_value.show()

sales_report = orders_df.select("order_date", "product_name", "total_amount")
sales_report.show(10)

sales_report_renamed = orders_df.select(
    col("order_date").alias("date"),
    col("product_name").alias("product"),
    col("total_amount").alias("revenue")
)
sales_report_renamed.show(10)

# Alternative approach
orders_renamed = orders_df.withColumnRenamed("total_amount", "revenue")
orders_renamed.show(5)

top_orders = orders_df.orderBy(col("total_amount").desc())
top_orders.show(10)

orders_sorted = orders_df.orderBy(
    col("category"),
    col("total_amount").desc()
)
orders_sorted.show(15)

from pyspark.sql.functions import sum, count, avg

country_revenue = orders_df.groupBy("country") \
    .agg(sum("total_amount").alias("total_revenue")) \
    .orderBy(col("total_revenue").desc())

country_revenue.show()

# Code - Multiple aggregations
country_stats = orders_df.groupBy("country").agg(
    count("order_id").alias("total_orders"),
    sum("total_amount").alias("total_revenue"),
    avg("total_amount").alias("avg_order_value")
).orderBy(col("total_revenue").desc())

country_stats.show()

from pyspark.sql.functions import round as spark_round

category_performance = orders_df.groupBy("category").agg(
    count("order_id").alias("total_orders"),
    sum("total_amount").alias("total_revenue"),
    spark_round(avg("total_amount"), 2).alias("avg_order_value")
).orderBy(col("total_revenue").desc())

category_performance.show()

from pyspark.sql.functions import round as spark_round

orders_with_discount = orders_df.withColumn(
    "discount_amount",
    spark_round(col("total_amount") * col("discount_percent") / 100, 2)
)

orders_with_discount.select("product_name", "total_amount", "discount_percent", "discount_amount").show()

# Add actual revenue
orders_with_metrics = orders_with_discount.withColumn(
    "actual_revenue",
    spark_round(col("total_amount") - col("discount_amount"), 2)
)

orders_with_metrics.select("product_name", "total_amount", "discount_amount", "actual_revenue").show()

from pyspark.sql.functions import month, year, dayofweek

orders_with_dates = orders_df.withColumn("order_month", month("order_date")) \
    .withColumn("order_year", year("order_date")) \
    .withColumn("day_of_week", dayofweek("order_date"))

orders_with_dates.select("order_date", "order_month", "order_year", "day_of_week", "total_amount").show()

# Monthly revenue trend
monthly_revenue = orders_with_dates.groupBy("order_year", "order_month").agg(
    sum("total_amount").alias("monthly_revenue"),
    count("order_id").alias("order_count")
).orderBy("order_year", "order_month")

monthly_revenue.show()

# Code - Count missing values
from pyspark.sql.functions import when, col

missing_count = orders_df.select(
    count(when(col("shipping_days").isNull(), 1)).alias("missing_shipping_days")
)
missing_count.show()

orders_cleaned = orders_df.dropna(subset=["shipping_days"])
print(f"Orders after dropping nulls: {orders_cleaned.count()}")

# Fill missing values with average
avg_shipping = orders_df.select(avg("shipping_days")).collect()[0][0]
print(f"Average shipping days: {round(avg_shipping, 2)}")

orders_filled = orders_df.fillna({"shipping_days": round(avg_shipping, 0)})
orders_filled.select("order_id", "shipping_days").show()

status_distribution = orders_df.groupBy("status").agg(
    count("order_id").alias("order_count"),
    sum("total_amount").alias("total_revenue")
).orderBy(col("order_count").desc())

status_distribution.show()

# Analyze cancelled orders
cancelled_orders = orders_df.filter(col("status") == "Cancelled")
cancelled_analysis = cancelled_orders.groupBy("category").agg(
    count("order_id").alias("cancelled_count"),
    spark_round(avg("total_amount"), 2).alias("avg_cancelled_value")
).orderBy(col("cancelled_count").desc())

cancelled_analysis.show()

payment_analysis = orders_df.groupBy("payment_method").agg(
    count("order_id").alias("transaction_count"),
    sum("total_amount").alias("total_value"),
    spark_round(avg("total_amount"), 2).alias("avg_transaction_value")
).orderBy(col("total_value").desc())

payment_analysis.show()

# Code - Customer lifetime value
customer_value = orders_df.groupBy("customer_id").agg(
    count("order_id").alias("total_orders"),
    sum("total_amount").alias("lifetime_value"),
    spark_round(avg("total_amount"), 2).alias("avg_order_value")
).orderBy(col("lifetime_value").desc())

customer_value.show(20)

# Add customer segmentation
from pyspark.sql.functions import when

customer_segments = customer_value.withColumn(
    "customer_segment",
    when(col("lifetime_value") > 1000, "VIP")
    .when(col("lifetime_value") > 500, "High Value")
    .when(col("lifetime_value") > 200, "Regular")
    .otherwise("New")
)

customer_segments.show(20)

from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

# Calculate revenue per product
product_revenue = orders_df.groupBy("category", "product_name").agg(
    sum("total_amount").alias("product_revenue")
)

# Create window specification
window_spec = Window.partitionBy("category").orderBy(col("product_revenue").desc())

# Add ranking
ranked_products = product_revenue.withColumn(
    "rank_in_category",
    row_number().over(window_spec)
)

ranked_products.show(20)

top_3_per_category = ranked_products.filter(col("rank_in_category") <= 3)
top_3_per_category.show()

# Calculate overall business metrics
total_revenue = orders_df.agg(sum("total_amount")).collect()[0][0]
total_orders_count = orders_df.count()
avg_order_value = orders_df.agg(avg("total_amount")).collect()[0][0]
total_customers = orders_df.select("customer_id").distinct().count()

# Print dashboard
print("=" * 60)
print("RETAILPRO E-COMMERCE DASHBOARD")
print("=" * 60)
print(f"Total Revenue: ${total_revenue:,.2f}")
print(f"Total Orders: {total_orders_count:,}")
print(f"Average Order Value: ${avg_order_value:.2f}")
print(f"Total Customers: {total_customers:,}")
print(f"Revenue per Customer: ${total_revenue/total_customers:.2f}")
print("=" * 60)

# Save top products to CSV
top_3_per_category.coalesce(1).write.mode("overwrite").csv("top_products_by_category", header=True)

# Save customer segments to Parquet (more efficient format)
customer_segments.write.mode("overwrite").parquet("customer_segments")

# Save country revenue to JSON
country_revenue.write.mode("overwrite").json("country_revenue")

!ls

!cat ecommerce_orders.csv

